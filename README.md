# Kasparro ETL & Backend System

- A production-grade ETL pipeline and FastAPI backend that ingests data from multiple sources, performs incremental processing, deduplication, and observability, and exposes APIs for downstream consumption.

- This project is designed to meet Backend & ETL Systems assignment requirements, following industry-grade architecture and best practices.

---

## 1. System Architecture

Data Sources
 ‚îú‚îÄ‚îÄ Public API
 ‚îú‚îÄ‚îÄ CSV File
 ‚îî‚îÄ‚îÄ Third Source (JSON)
        ‚Üì
Raw Data Layer (Audit & Traceability)
        ‚Üì
Incremental ETL (Checkpointed)
        ‚Üì
Unified & Deduplicated Data Model
        ‚Üì
FastAPI Backend
        ‚Üì
Consumers / Monitoring / Metrics




### Key Design Principles

- Raw data is always preserved (P0 requirement)

- Incremental ingestion via checkpoints (P1 requirement)

- Idempotent ETL runs

- Observability through Prometheus metrics

- Clear separation of concerns


## 2. üóÇ Project Structure

.
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app & startup ETL
‚îÇ   ‚îú‚îÄ‚îÄ routes.py            # API endpoints
‚îÇ   ‚îî‚îÄ‚îÄ dependencies/
‚îÇ       ‚îî‚îÄ‚îÄ auth.py          # API key authentication
‚îÇ
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Environment configuration
‚îÇ   ‚îú‚îÄ‚îÄ database.py          # DB engine & session management
‚îÇ   ‚îî‚îÄ‚îÄ models.py            # SQLAlchemy models
‚îÇ
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ api_source.py        # Public API ingestion
‚îÇ   ‚îú‚îÄ‚îÄ csv_source.py        # CSV ingestion
‚îÇ   ‚îú‚îÄ‚îÄ third_source.py      # Third source ingestion
‚îÇ   ‚îî‚îÄ‚îÄ etl_runner.py        # ETL orchestration
‚îÇ
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îî‚îÄ‚îÄ unified.py           # Pydantic schemas
‚îÇ
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # Prometheus metrics
‚îÇ   ‚îú‚îÄ‚îÄ rate_limiter.py      # Rate limiting & retries
‚îÇ   ‚îî‚îÄ‚îÄ schema_drift.py      # Schema drift detection
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
‚îÇ   ‚îî‚îÄ‚îÄ test_etl.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ sample.csv
‚îÇ   ‚îî‚îÄ‚îÄ third_source.json
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md


## 3. Features

### ETL Capabilities

- Multi-source ingestion (API, CSV, JSON)

- Raw data persistence for auditability

- Incremental ingestion using checkpoints

- Canonical normalization & deduplication

- Idempotent ETL re-runs

- Success / Partial / Failure classification

- ETL execution audit logging

### Backend Capabilities

- Health check endpoint

- Paginated data access API

- ETL statistics API

- Prometheus-compatible metrics

- API key‚Äìbased authentication

### Reliability & Observability

- Rate-limited external API ingestion
- Retry with exponential backoff
- Prometheus counters & histograms
- Safe ETL execution on application startup
---

## 4. Data Model

### Raw Tables
- `raw_api_data`
- `raw_csv_data`
- `raw_third_source_data`

### Control Tables
- `etl_checkpoint`
- `etl_runs`

### Unified Table
- `etl_checkpoint`
- `etl_runs`
#### These tables support:
- `Incremental ingestion`
- `Auditing`
- `Operational transparency`

### Unified Table (Canonical Output)
- `unified_data`

| Column        | Purpose                                    |
| ------------- | ------------------------------------------ |
| `name`        | **Canonical entity identity (normalized)** |
| `value`       | Business value                             |
| `source`      | Metadata only (not part of identity)       |
| `ingested_at` | Timestamp                                  |


#### Design decision:
- Identity is enforced only on the canonical name
- Source is treated as metadata, not identity
- This ensures true unification across sources

---

## 5. Canonical Normalization Strategy
- All incoming records are normalized before insertion:
def normalize_name(name: str) -> str:
    return name.strip().lower()

## 6. API Endpoints
### Root
- Returns application status.

- `GET /`
### Response:
- `{ "status": "running" }`

### Health Check
- `GET /health`
### Response:
- `{`
  `"status": "ok",`
  `"last_etl": "success | partial | failed"`
`}`

### Data API (Authentication Required)
- `GET /data`
- `Headers:`
`  x-api-key: `<API_KEY>
`Query Params:`
`  limit, offset, source`

### ETL Statistics
- `GET /stats`

#### Response includes:
- `Last run timestamp`
- `Last success timestamp`
- `Last failure timestamp`

### Metrics
- `GET /metrics`
#### Prometheus-compatible metrics including:
- `etl_runs_total`
- `etl_failures_total`
- `etl_records_total`
- `etl_duration_seconds`

## 7. Security & Secrets
- No secrets are committed
- `.env` is git-ignored
- `.env.example` provided
- API protected via API key
- Database credentials injected via environment variables


## 8. Deployment
Live Deployment (Railway)
#### URL: 
- `https://kasparro-backend-lalan-kumar-production.up.railway.app`

#### Infrastructure
- FastAPI hosted on Railway
- PostgreSQL hosted on Railway
- Docker-based deployment
- Non-root container user

### Startup Behavior
- Database initialized
- Tables created if missing
- ETL executed safely on startup
- Failures do not block API availability

## 9. Running Locally
- `docker-compose up --build`

#### and:
- `http://localhost:8000
`
## 10. Environment Variables
### Create a .env file:
- `DATABASE_URL=postgresql://user:password@host:port/dbname`
- `API_KEY=your-secret-key`

## 11. Running the Project
### ‚ñ∂Ô∏è Local (Python)
- pip install -r requirements.txt
- uvicorn api.main:app --reload
### üê≥ Docker
- docker-compose up --build



## 12. Testing
- pytest

### Tests cover:
- API endpoints
- ETL execution
- Incremental ingestion
- Failure handling

## 13. Author
- Lalan Kumar



